{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import Tokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_bilstm_crf:\n",
    "    def __init__(self, max_seq_length, batch_size, epochs, lstm_dim):\n",
    "        self.label = {}\n",
    "        self._label = {}\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lstmDim = lstm_dim\n",
    "        self.label_path = r\"uncased_L-2_H-128_A-2\\tag_dict.txt\"\n",
    "        self.vocab_path = r\"uncased_L-2_H-128_A-2\\vocab.txt\"\n",
    "        self.model_path = r\"uncased_L-2_H-128_A-2/\"\n",
    "        \n",
    "        self.LoadLabel()\n",
    "        self.model = self.Model()\n",
    "        self.model.load_weights('NER_model/my_NER_model')\n",
    "        \n",
    "    ##############################################\n",
    "    def LoadLabel(self):\n",
    "        #label\n",
    "        f_label = open(self.label_path, 'r+', encoding='utf-8')\n",
    "        for line in f_label:\n",
    "            content = line.strip().split()\n",
    "            self.label[content[0].strip()] = content[1].strip()\n",
    "            self._label[content[1].strip()] = content[0].strip()\n",
    "            \n",
    "        #dict\n",
    "        self.vocab = {}\n",
    "        with open(self.vocab_path, 'r+', encoding='utf-8') as f_vocab:\n",
    "            for line in f_vocab.readlines():\n",
    "                self.vocab[line.strip()] = len(self.vocab)\n",
    "\n",
    "    def Model(self):\n",
    "        bert = load_trained_model_from_checkpoint(\n",
    "            self.model_path + \"bert_config.json\",\n",
    "            self.model_path + \"bert_model.ckpt\",\n",
    "            seq_len=self.max_seq_length\n",
    "            )\n",
    "        #make bert layer trainable\n",
    "        for layer in bert.layers:\n",
    "            layer.trainable = True\n",
    "        x1 = Input(shape=(None,))\n",
    "        x2 = Input(shape=(None,))\n",
    "        bert_out = bert([x1, x2])\n",
    "        lstm_out = Bidirectional(LSTM(self.lstmDim,\n",
    "                                         return_sequences=True,\n",
    "                                         dropout=0.2,\n",
    "                                         recurrent_dropout=0.2))(bert_out)\n",
    "        crf_out = CRF(len(self.label), sparse_target=True)(lstm_out)\n",
    "        model = Model([x1, x2], crf_out)\n",
    "        model.summary()\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(1e-4),\n",
    "            loss=crf_loss,\n",
    "            metrics=[crf_accuracy]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def PreProcessInputData(self, text):\n",
    "        word_labels = []\n",
    "        seq_types = []\n",
    "        \n",
    "        for sequence in text:\n",
    "            len_text = len(sequence)\n",
    "            \n",
    "            ###########################################\n",
    "            temp_word_labels = []\n",
    "            \n",
    "            temp_word_labels.append( 101 )            \n",
    "            for w in sequence:\n",
    "                temp_word_labels.append( self.vocab.get(w,1) )\n",
    "            temp_word_labels.append( 102 )\n",
    "            \n",
    "            ###########################################\n",
    "            temp_seq_types = [1] * len(temp_word_labels) +  [0] * (self.max_seq_length - len( temp_word_labels ))\n",
    "            temp_word_labels = temp_word_labels + [0] * (self.max_seq_length - len( temp_word_labels ))\n",
    "            \n",
    "            word_labels.append( temp_word_labels )\n",
    "            seq_types.append( temp_seq_types )\n",
    "            \n",
    "        return word_labels, seq_types\n",
    "\n",
    "\n",
    "    def PreProcessOutputData(self, text):\n",
    "        tags = []\n",
    "        for line in text:\n",
    "            tag = [0]\n",
    "            for item in line:\n",
    "                tag.append(int(self.label[item.strip()]))\n",
    "            tag.append(0)\n",
    "            tags.append(tag)\n",
    "\n",
    "        pad_tags = pad_sequences(tags, maxlen=self.max_seq_length, padding=\"post\", truncating=\"post\")\n",
    "        result_tags = np.expand_dims(pad_tags, 2)\n",
    "        return result_tags\n",
    "\n",
    "    def TrainModel(self, train_data):\n",
    "        input_train, result_train = train_data\n",
    "        input_test, result_test = test_data\n",
    "        \n",
    "        #训练集\n",
    "        input_train_labels, input_train_types = self.PreProcessInputData(input_train)\n",
    "        result_train = self.PreProcessOutputData(result_train)\n",
    "        \n",
    "        #测试集\n",
    "#         input_test_labels, input_test_types = self.PreProcessInputData(input_test)\n",
    "#         result_test = self.PreProcessOutputData(result_test)\n",
    "        \n",
    "        history = self.model.fit(x=[input_train_labels, input_train_types],\n",
    "                       y=result_train,\n",
    "                       validation_split=0.2,\n",
    "                       batch_size=self.batch_size,\n",
    "                       epochs=self.epochs,\n",
    "                       shuffle=True,\n",
    "                       verbose=1,\n",
    "                       class_weight = 'auto')\n",
    "        return\n",
    "\n",
    "    def Id2Label(self, ids):\n",
    "        result = []\n",
    "        for id in ids:\n",
    "            result.append(self._label[str(id)])\n",
    "        return result\n",
    "\n",
    "    def Vector2Id(self, tags):\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            result.append(np.argmax(tag))\n",
    "        return result\n",
    "\n",
    "    def ModelPredict(self, sentence):\n",
    "        labels, types = self.PreProcessInputData([sentence])\n",
    "        tags = self.model.predict([labels, types])[0]\n",
    "        \n",
    "        result = []\n",
    "        for i in range(1, len(sentence) + 1):\n",
    "            result.append(tags[i])\n",
    "        result = self.Vector2Id(result)\n",
    "        tag = self.Id2Label(result)\n",
    "        return tag\n",
    "\n",
    "    def EvalModel(self, valid_data):\n",
    "        input_valid, result_valid = valid_data\n",
    "        #训练集\n",
    "        input_valid_labels, input_valid_types = self.PreProcessInputData(input_valid)\n",
    "        result_valid = self.PreProcessOutputData(result_valid)\n",
    "        \n",
    "        res = ( self.model.evaluate(x=[input_valid_labels, input_valid_types],\n",
    "                           y=result_valid,batch_size=self.batch_size) )\n",
    "        print(res)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 multiple             4320256     input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, None, 128)    98816       model_8[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, None, 5)      680         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 4,419,752\n",
      "Trainable params: 4,419,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "lstmDim = 64\n",
    "\n",
    "model = bert_bilstm_crf( max_seq_length, batch_size, epochs, lstmDim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predict( model, tem):\n",
    "    predict_list = model.ModelPredict( [w.lower() for w in tem] )\n",
    "\n",
    "    result_list = []\n",
    "    tem_skill = ''\n",
    "    \n",
    "#     print()\n",
    "#     print( '【Extract Result】', end='' )\n",
    "    i = 0\n",
    "    \n",
    "    pre = ''\n",
    "    for i in range( len( predict_list ) ):\n",
    "        if predict_list[i] == 'B':\n",
    "            if tem_skill != '':\n",
    "                result_list.append( tem_skill )\n",
    "                tem_skill = ''\n",
    "                \n",
    "#             print( '|',tem[i],'',end='' )\n",
    "            tem_skill = tem[i]\n",
    "            \n",
    "        if predict_list[i] == 'I':\n",
    "#             print( tem[i],'',end='' )\n",
    "            tem_skill += ' ' + tem[i]\n",
    "            \n",
    "        if predict_list[i] == '0':\n",
    "            if pre == 'B' or pre == 'I':\n",
    "                result_list.append( tem_skill )\n",
    "                tem_skill = ''\n",
    "            tem_skill = ''\n",
    "            \n",
    "        pre = predict_list[i]\n",
    "        \n",
    "    if tem_skill != '':\n",
    "        result_list.append( tem_skill )\n",
    "        \n",
    "    result_list = [w.strip() for w in result_list if w.strip() != '']\n",
    "    print()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_DF = pd.read_csv(r'../Data/JD.csv')\n",
    "JD_DF = JD_DF[JD_DF['Query'] == 'Java Developer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Qualifications: The successful candidate will have Object-Oriented experience in these areas: • Spring Framework • Hibernate/JPA • Web Services in java • Java Swing Knowledge The successful candidate will also have a good working knowledge of the following: • C, C++, Make, Makefiles • Java Core API’s • Using Subversion • Web Logic • Unix Scripts knowledge a plus (.ksh,. bsh,. csh) • COBOL knowledge is a plus • Working knowledge of Perl Scripts • Excellent knowledge of Oracle SQL, must be able to read, write and debug Oracle scripts. This position requires the ability to multi-task in a high-paced environment.\\xa0\\xa0\\\\r\\\\rResponsibilities: Build an application with persistent domain objects using Dependency Injection, AOP (AspectJ) and Spring Object Relational Mapping (ORM with JPA/Hibernate) and with Ant build files.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = JD_DF.sample(1).iloc[0]['Description']\n",
    "soup = BeautifulSoup(text)\n",
    "text = soup.get_text()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualifications: The successful candidate will have Object-Oriented experience in these areas: • Spring Framework • Hibernate/JPA • Web Services in java • Java Swing Knowledge The successful candidate will also have a good working knowledge of the following: • C, C++, Make, Makefiles • Java Core API’s • Using Subversion • Web Logic • Unix Scripts knowledge a plus (.ksh,. bsh,. csh) • COBOL knowledge is a plus • Working knowledge of Perl Scripts • Excellent knowledge of Oracle SQL, must be able to read, write and debug Oracle scripts. This position requires the ability to multi-task in a high-paced environment.  \\r\\rResponsibilities: Build an application with persistent domain objects using Dependency Injection, AOP (AspectJ) and Spring Object Relational Mapping (ORM with JPA/Hibernate) and with Ant build files.\n",
      "========================================================================================================================\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Qualifications: The successful candidate will have Object-Oriented experience in these areas: • Spring Framework • Hibernate/JPA • Web Services in java • Java Swing Knowledge The successful candidate will also have a good working knowledge of the following: • C, C++, Make, Makefiles • Java Core API’s • Using Subversion • Web Logic • Unix Scripts knowledge a plus (.ksh,. bsh,. csh) • COBOL knowledge is a plus • Working knowledge of Perl Scripts • Excellent knowledge of Oracle SQL, must be able to read, write and debug Oracle scripts. This position requires the ability to multi-task in a high-paced environment.\n",
      "\n",
      "['Hibernate', 'JPA', 'java', 'C++', 'csh SQL']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Responsibilities: Build an application with persistent domain objects using Dependency Injection, AOP (AspectJ) and Spring Object Relational Mapping (ORM with JPA/Hibernate) and with Ant build files.\n",
      "\n",
      "['objects', 'Dependency Injection', 'AOP', 'AspectJ Object Relational Mapping', 'JPA', 'Hibernate files']\n",
      "========================================================================================================================\n",
      "['AOP', 'java', 'Hibernate', 'AspectJ Object Relational Mapping', 'Hibernate files', 'JPA', 'Dependency Injection', 'objects', 'csh SQL', 'C++']\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print('='*120)\n",
    "\n",
    "total_skill = []\n",
    "for sentence in text.replace('\\\\t',' ').replace('\\\\r','\\\\n').split('\\\\n'):\n",
    "    if len(sentence.strip()) > 0:\n",
    "        print('-'*120)\n",
    "        \n",
    "        print( sentence.strip() )\n",
    "        tem_list = print_predict( model, [str(w) for w in list(nlp(sentence.strip()))[:120] ] ) \n",
    "        print( tem_list )\n",
    "        \n",
    "        total_skill += tem_list\n",
    "\n",
    "print('='*120)\n",
    "print( list(set(total_skill)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
